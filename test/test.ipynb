{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print('Hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print('Hello World!')\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.57 μs ± 57.7 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit [j**4 for j in range(1,99)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "805 ns ± 10.2 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "%timeit np.arange(1, 99)**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9  8 40  6  5]\n",
      "[ 9  8 40  6  5]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([9,8,7,6,5])\n",
    "\n",
    "vi = x.view()\n",
    "\n",
    "vi[2] =40\n",
    "\n",
    "print(x)\n",
    "print(vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add():\n",
    "    x = int(input(\"Enter the first number: \"))\n",
    "    y = int(input(\"Enter the second number: \"))\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = add()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Employee:\n",
    "    def __init__(self, name, employee_id, position, salary):\n",
    "        self.name = name\n",
    "        self.employee_id = employee_id\n",
    "        self.position = position\n",
    "        self.salary = salary\n",
    "\n",
    "    def display_info(self):\n",
    "        print(f\"Name: {self.name}\")\n",
    "        print(f\"Employee ID: {self.employee_id}\")\n",
    "        print(f\"Position: {self.position}\")\n",
    "        print(f\"Salary: {self.salary}\")\n",
    "\n",
    "    def give_raise(self, raiseamount):\n",
    "       self.salary += raiseamount\n",
    "        \n",
    "    \n",
    "    \n",
    "    def give_performance_raise(self, performance_rating):\n",
    "        if performance_rating == \"Excellent\":\n",
    "            self.salary =+ self.salary*1.10\n",
    "        elif performance_rating ==\"Good\":\n",
    "            self.salary =+ self.salary*1.07\n",
    "        elif performance_rating == \"Average\":\n",
    "            self.salary =+ self.salary*1.03\n",
    "        elif performance_rating == \"Poor\":\n",
    "            self.salary = self.salary\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp1 = Employee(\"John Doe\", 101, \"Developer\", 60000)\n",
    "emp2 = Employee(\"Jane Smith\", 102, \"Designer\", 55000)\n",
    "\n",
    "emp1.give_performance_raise(\"Excellent\")\n",
    "emp2.give_performance_raise(\"Average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # John's salary should now be 65000\n",
    "emp1.give_raise(5000)\n",
    "emp2.give_raise(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n"
     ]
    }
   ],
   "source": [
    "squares = [x**2 for x in range(10)]\n",
    "print(squares)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The operation couldn’t be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript\n",
      "head: illegal line count -- -1\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize SparkSession\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create a DataFrame with a range of numbers\u001b[39;00m\n\u001b[1;32m      7\u001b[0m myrange \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mrange(\u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m.\u001b[39mtoDF(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Create a DataFrame with a range of numbers\n",
    "myrange = spark.range(1000).toDF(\"number\")\n",
    "\n",
    "# Show the DataFrame\n",
    "myrange.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GnuPG in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: psutil>=1.2.1 in /Users/dashrath/Library/Python/3.12/lib/python/site-packages (from GnuPG) (6.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install GnuPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "GnuPG is not installed!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgnupg\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize GPG\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m gpg \u001b[38;5;241m=\u001b[39m \u001b[43mgnupg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGPG\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Encrypt a file\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_file.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/gnupg/gnupg.py:117\u001b[0m, in \u001b[0;36mGPG.__init__\u001b[0;34m(self, binary, homedir, verbose, use_agent, keyring, secring, ignore_homedir_permissions, options)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, homedir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     62\u001b[0m              use_agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, keyring\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, secring\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     63\u001b[0m              ignore_homedir_permissions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize a GnuPG process wrapper.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    :param str binary: Name for GnuPG binary executable. If the absolute\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    '/usr/bin/gpg'\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mGPG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhome\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhomedir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeyring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeyring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43msecring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msecring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_homedir_permissions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_homedir_permissions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124m    Initialised settings:\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124m    binary: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m            \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose),\n\u001b[1;32m    150\u001b[0m            \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_agent))))\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhomedir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch-files\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/gnupg/_meta.py:182\u001b[0m, in \u001b[0;36mGPGBase.__init__\u001b[0;34m(self, binary, home, keyring, secring, use_agent, default_preference_list, ignore_homedir_permissions, verbose, options)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a ``GPGBase``.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03mThis class is used to set up properties for controlling the behaviour\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m                   file for secret keys.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_homedir_permissions \u001b[38;5;241m=\u001b[39m ignore_homedir_permissions\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary  \u001b[38;5;241m=\u001b[39m \u001b[43m_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_binary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhomedir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(home) \u001b[38;5;28;01mif\u001b[39;00m home \u001b[38;5;28;01melse\u001b[39;00m _util\u001b[38;5;241m.\u001b[39m_conf\n\u001b[1;32m    184\u001b[0m pub \u001b[38;5;241m=\u001b[39m _parsers\u001b[38;5;241m.\u001b[39m_fix_unsafe(keyring) \u001b[38;5;28;01mif\u001b[39;00m keyring \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpubring.gpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/gnupg/_util.py:429\u001b[0m, in \u001b[0;36m_find_binary\u001b[0;34m(binary)\u001b[0m\n\u001b[1;32m    427\u001b[0m             log\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find binary for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpg2\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m found \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGnuPG is not installed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m found\n",
      "\u001b[0;31mRuntimeError\u001b[0m: GnuPG is not installed!"
     ]
    }
   ],
   "source": [
    "import gnupg\n",
    "\n",
    "# Initialize GPG\n",
    "gpg = gnupg.GPG()\n",
    "\n",
    "# Encrypt a file\n",
    "with open('input_file.txt', 'rb') as f:\n",
    "    status = gpg.encrypt_file(f, recipients=['recipient@example.com'], output='encrypted_file.gpg')\n",
    "\n",
    "# Check if encryption was successful\n",
    "if status.ok:\n",
    "    print('File encrypted successfully.')\n",
    "else:\n",
    "    print('Encryption failed:', status.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test unit\n"
     ]
    }
   ],
   "source": [
    "print(\"test unit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The operation couldn’t be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript\n",
      "head: illegal line count -- -1\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create a SparkSession\u001b[39;00m\n\u001b[1;32m      4\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'PGPASS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#get password from environmnet var\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m pwd \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPGPASS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m uid \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPGUID\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#sql db details\u001b[39;00m\n",
      "File \u001b[0;32m<frozen os>:714\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'PGPASS'"
     ]
    }
   ],
   "source": [
    "#import needed libraries\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import URL\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#get password from environmnet var\n",
    "pwd = os.environ['PGPASS']\n",
    "uid = os.environ['PGUID']\n",
    "#sql db details\n",
    "driver = \"{SQL Server Native Client 11.0}\"\n",
    "server = \"localhost\"\n",
    "database = \"AdventureWorksDW2019;\"\n",
    "\n",
    "#extract data from sql server\n",
    "def extract():\n",
    "    try:\n",
    "        connection_string = 'DRIVER=' + driver + ';SERVER=' + server + ';DATABASE=' + database + ';UID=' + uid + ';PWD=' + pwd\n",
    "        connection_url = URL.create(\"mssql+pyodbc\", query={\"odbc_connect\": connection_string})\n",
    "        src_engine = create_engine(connection_url)\n",
    "        src_conn = src_engine.connect()\n",
    "        # execute query\n",
    "        query = \"\"\" select  t.name as table_name\n",
    "        from sys.tables t where t.name in ('DimProduct','DimProductSubcategory','DimProductSubcategory','DimProductCategory','DimSalesTerritory','FactInternetSales') \"\"\"\n",
    "        src_tables = pd.read_sql_query(query, src_conn).to_dict()['table_name']\n",
    "\n",
    "        for id in src_tables:\n",
    "            table_name = src_tables[id]\n",
    "            df = pd.read_sql_query(f'select * FROM {table_name}', src_conn)\n",
    "            load(df, table_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Data extract error: \" + str(e))\n",
    "\n",
    "#load data to postgres\n",
    "def load(df, tbl):\n",
    "    try:\n",
    "        rows_imported = 0\n",
    "        engine = create_engine(f'postgresql://{uid}:{pwd}@{server}:5432/adventureworks')\n",
    "        print(f'importing rows {rows_imported} to {rows_imported + len(df)}... for table {tbl}')\n",
    "        # save df to postgres\n",
    "        df.to_sql(f'stg_{tbl}', engine, if_exists='replace', index=False, chunksize=100000)\n",
    "        rows_imported += len(df)\n",
    "        # add elapsed time to final print out\n",
    "        print(\"Data imported successful\")\n",
    "    except Exception as e:\n",
    "        print(\"Data load error: \" + str(e))\n",
    "\n",
    "try:\n",
    "    #call extract function\n",
    "    extract()\n",
    "except Exception as e:\n",
    "    print(\"Error while extracting data: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(\"Salary\")\n",
    "df.withColumn('row_number', row_number().over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows with any null values\n",
    "df_clean = df.dropna()\n",
    "\n",
    "#fill null values with a default value\n",
    "df_filled = df.fillna({'age':0, 'name':'dashbro'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Step 1: Create Spark Sesssion\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"KafkaSparkStreaming\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Step 2: Read data from Kafka\n",
    "df = spark\\\n",
    "    .readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"Subscribe\",\"input-topic\")\\\n",
    "    .load()\n",
    "\n",
    "# Step 3: Process data\n",
    "df = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "\n",
    "#Step 4: Write data to sink\n",
    "query = df\\\n",
    "    .writeStream\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .format(\"parquet\")\\\n",
    "    .option(\"path\",\"/path/to/output\")\\\n",
    "    .option(\"checkpointLocation\", \"/path/to/checkpoint\")\\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermication()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_string(s):\n",
    "    return s[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iH\n"
     ]
    }
   ],
   "source": [
    "print(reverse_string('Hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_new(s):\n",
    "    empty_str =''\n",
    "\n",
    "    for char in s:\n",
    "        empty_str = char + empty_str\n",
    "\n",
    "    return empty_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "olleh\n"
     ]
    }
   ],
   "source": [
    "print(reverse_new('hello'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordered a  pizza with following toppings\n",
      "-ExtraLarge\n",
      "-pepporoni\n",
      "-olives\n",
      "\n",
      " Details of order are:\n",
      " -delivery,True\n",
      " -tip,5\n"
     ]
    }
   ],
   "source": [
    "def pizz_delivery(size, *toppings, **details):\n",
    "    print(f\"Ordered a {size} pizza with following toppings\")\n",
    "    for topping in toppings:\n",
    "        print(f\"-{topping}\")\n",
    "    print(f\"\\n Details of order are:\")\n",
    "    for key, value in details.items():\n",
    "        print(f\" -{key},{value}\")\n",
    "\n",
    "pizz_delivery(\"ExtraLarge\",\"pepporoni\",\"olives\",delivery=True, tip=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
